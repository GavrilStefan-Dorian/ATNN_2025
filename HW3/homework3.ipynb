{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf575099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adf4c694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 19 22:39:42 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   46C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi # test we are on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76639f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.9.1+cu126\n",
      "Uninstalling torch-2.9.1+cu126:\n",
      "  Successfully uninstalled torch-2.9.1+cu126\n",
      "Found existing installation: torchaudio 2.9.1+cu126\n",
      "Uninstalling torchaudio-2.9.1+cu126:\n",
      "  Successfully uninstalled torchaudio-2.9.1+cu126\n",
      "Found existing installation: torchvision 0.24.1+cu126\n",
      "Uninstalling torchvision-0.24.1+cu126:\n",
      "  Successfully uninstalled torchvision-0.24.1+cu126\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fastai 2.8.5 requires torchvision>=0.11, which is not installed.\n",
      "timm 1.0.22 requires torchvision, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "%pip uninstall -y torch torchaudio torchvision\n",
    "\n",
    "%pip install torch==2.9.1 --index-url https://download.pytorch.org/whl/test/cu126 -q # 2.9.1 has Muon\n",
    "%pip install torchaudio==2.9.1 --index-url https://download.pytorch.org/whl/test/cu126 -q \n",
    "\n",
    "%pip install torchvision==0.24.1+cu126 --index-url https://download.pytorch.org/whl/cu126 -q\n",
    "\n",
    "%pip install timm wandb==0.22.0 torchmetrics numpy tensorboard matplotlib -q #--no-deps -q # no-deps to avoid torch 2.8.0, but wandb needs newer vers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a2b6776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from torchvision.transforms import v2\n",
    "from torch.backends import cudnn\n",
    "from torch import GradScaler\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from torchvision.datasets import CIFAR10, CIFAR100, MNIST, OxfordIIITPet\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
    "import timm\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5713e38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2.9.1+cu126\n"
     ]
    }
   ],
   "source": [
    "print(\"Muon\" in dir(torch.optim))\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd77abcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad scaler is enabled: True\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.accelerator.current_accelerator() if torch.accelerator.is_available() else torch.device(\"cpu\")\n",
    "enable_half = device.type != \"cpu\"\n",
    "scaler = GradScaler(device, enabled=enable_half)\n",
    "\n",
    "print(\"Grad scaler is enabled:\", enable_half)\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5f0bec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"batch_size\": 64,\n",
    "    \"num_workers\": 2,\n",
    "    \"pin_memory\": True,\n",
    "    \"epochs\": 50,\n",
    "    \"lr\": 1e-1,\n",
    "    \"model\": \"resnet18\",\n",
    "    \"image_size\": 224,\n",
    "    \"use_amp\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05bb8972",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_transforms = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "])\n",
    "\n",
    "train_transforms = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.RandomCrop(32, padding=4),\n",
    "    v2.RandomHorizontalFlip(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    # v2.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
    "])\n",
    "\n",
    "test_transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    # v2.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
    "])\n",
    "\n",
    "train_dataset= CIFAR100(root='./data', train=True, download=True, transform=train_transforms)\n",
    "test_dataset = CIFAR100(root='./data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True, num_workers=config[\"num_workers\"], pin_memory=config[\"pin_memory\"])\n",
    "test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False, num_workers=config[\"num_workers\"], pin_memory=config[\"pin_memory\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29f9983d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act1): ReLU(inplace=True)\n",
       "  (maxpool): Identity()\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
       "  (fc): Linear(in_features=512, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = timm.create_model(config[\"model\"], pretrained=False, num_classes=100)\n",
    "\n",
    "model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "model.maxpool = nn.Identity()\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ad3bdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    return train_loss / len(train_loader), 100.0 * correct/total\n",
    "\n",
    "\n",
    "def test(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    return test_loss / len(test_loader), 100.0 * correct/total\n",
    "\n",
    "def train_model(train_loader, test_loader, model, epochs=config[\"epochs\"], batch_size=config[\"batch_size\"], lr=config[\"lr\"]):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "    scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    writer = SummaryWriter(log_dir=f'./logs/cifar100')\n",
    "\n",
    "    best_acc = 0.0\n",
    "\n",
    "    best_loss = None\n",
    "    counter = 0\n",
    "    patience = 10\n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "    print(\"Start training\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        test_loss, test_acc = test(model, test_loader, criterion, device)\n",
    "\n",
    "        scheduler.step()\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        epoch_time = time.time() - epoch_start\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%, LR: {current_lr:.6f}, Time: {epoch_time:.2f}s\")\n",
    "\n",
    "        writer.add_scalar('Loss/Train', train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Test', test_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Train', train_acc, epoch)\n",
    "        writer.add_scalar('Accuracy/Test', test_acc, epoch)\n",
    "        writer.add_scalar('Learning Rate', current_lr, epoch)\n",
    "\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            os.makedirs('./checkpoints', exist_ok=True)\n",
    "\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_acc': best_acc,        \n",
    "            }, f'./checkpoints/cifar_best_model.pth')\n",
    "            print(f\"Best model saved with accuracy: {best_acc:.2f}%\")\n",
    "\n",
    "        if best_loss is None:\n",
    "            best_loss = test_loss\n",
    "        elif test_loss > best_loss:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f\"Early stopping triggered for epoch {epoch+1}\")\n",
    "                break\n",
    "        else:\n",
    "            best_loss = test_loss\n",
    "            counter = 0\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Training completed in {total_time/60:.2f} minutes\")\n",
    "    print(f\"Best Test Accuracy: {best_acc:.2f}%\")\n",
    "    \n",
    "    writer.close()\n",
    "\n",
    "    return {\n",
    "        'best_acc': best_acc,\n",
    "        'total_time': total_time,\n",
    "        'final_train_acc': train_acc,\n",
    "        'final_test_acc': test_acc\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14e204f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "Epoch 1/50, Train Loss: 3.8425, Train Acc: 10.91%, Test Loss: 3.4793, Test Acc: 16.24%, LR: 0.100000, Time: 43.19s\n",
      "Best model saved with accuracy: 16.24%\n",
      "Epoch 2/50, Train Loss: 3.1362, Train Acc: 22.36%, Test Loss: 3.0392, Test Acc: 24.33%, LR: 0.100000, Time: 44.53s\n",
      "Best model saved with accuracy: 24.33%\n",
      "Epoch 3/50, Train Loss: 2.5881, Train Acc: 32.67%, Test Loss: 2.4548, Test Acc: 35.74%, LR: 0.100000, Time: 44.84s\n",
      "Best model saved with accuracy: 35.74%\n",
      "Epoch 4/50, Train Loss: 2.2893, Train Acc: 39.08%, Test Loss: 2.5421, Test Acc: 34.62%, LR: 0.100000, Time: 45.54s\n",
      "Epoch 5/50, Train Loss: 2.1054, Train Acc: 42.89%, Test Loss: 2.2748, Test Acc: 40.59%, LR: 0.100000, Time: 44.62s\n",
      "Best model saved with accuracy: 40.59%\n",
      "Epoch 6/50, Train Loss: 1.9659, Train Acc: 46.09%, Test Loss: 2.1633, Test Acc: 43.25%, LR: 0.100000, Time: 44.77s\n",
      "Best model saved with accuracy: 43.25%\n",
      "Epoch 7/50, Train Loss: 1.8708, Train Acc: 48.60%, Test Loss: 1.9918, Test Acc: 46.57%, LR: 0.100000, Time: 44.55s\n",
      "Best model saved with accuracy: 46.57%\n",
      "Epoch 8/50, Train Loss: 1.8010, Train Acc: 50.31%, Test Loss: 1.9883, Test Acc: 46.94%, LR: 0.100000, Time: 45.05s\n",
      "Best model saved with accuracy: 46.94%\n",
      "Epoch 9/50, Train Loss: 1.7544, Train Acc: 51.54%, Test Loss: 1.9320, Test Acc: 47.77%, LR: 0.100000, Time: 45.28s\n",
      "Best model saved with accuracy: 47.77%\n",
      "Epoch 10/50, Train Loss: 1.7122, Train Acc: 52.31%, Test Loss: 2.1774, Test Acc: 44.20%, LR: 0.100000, Time: 44.52s\n"
     ]
    }
   ],
   "source": [
    "results = train_model(train_loader, test_loader, model, config[\"epochs\"], config[\"batch_size\"], config[\"lr\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
