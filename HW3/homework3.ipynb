{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf575099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adf4c694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 19 22:39:42 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   46C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi # test we are on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76639f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.9.1+cu126\n",
      "Uninstalling torch-2.9.1+cu126:\n",
      "  Successfully uninstalled torch-2.9.1+cu126\n",
      "Found existing installation: torchaudio 2.9.1+cu126\n",
      "Uninstalling torchaudio-2.9.1+cu126:\n",
      "  Successfully uninstalled torchaudio-2.9.1+cu126\n",
      "Found existing installation: torchvision 0.24.1+cu126\n",
      "Uninstalling torchvision-0.24.1+cu126:\n",
      "  Successfully uninstalled torchvision-0.24.1+cu126\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fastai 2.8.5 requires torchvision>=0.11, which is not installed.\n",
      "timm 1.0.22 requires torchvision, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "%pip uninstall -y torch torchaudio torchvision\n",
    "\n",
    "%pip install torch==2.9.1 --index-url https://download.pytorch.org/whl/test/cu126 -q # 2.9.1 has Muon\n",
    "%pip install torchaudio==2.9.1 --index-url https://download.pytorch.org/whl/test/cu126 -q \n",
    "\n",
    "%pip install torchvision==0.24.1+cu126 --index-url https://download.pytorch.org/whl/cu126 -q\n",
    "\n",
    "%pip install timm wandb==0.22.0 torchmetrics numpy tensorboard matplotlib -q #--no-deps -q # no-deps to avoid torch 2.8.0, but wandb needs newer vers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a2b6776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from torchvision.transforms import v2\n",
    "from torch.backends import cudnn\n",
    "from torch import GradScaler\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from torchvision.datasets import CIFAR10, CIFAR100, MNIST, OxfordIIITPet\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
    "import timm\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5713e38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2.9.1+cu126\n"
     ]
    }
   ],
   "source": [
    "print(\"Muon\" in dir(torch.optim))\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd77abcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad scaler is enabled: True\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.accelerator.current_accelerator() if torch.accelerator.is_available() else torch.device(\"cpu\")\n",
    "enable_half = device.type != \"cpu\"\n",
    "scaler = GradScaler(device, enabled=enable_half)\n",
    "\n",
    "print(\"Grad scaler is enabled:\", enable_half)\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5f0bec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"batch_size\": 64,\n",
    "    \"num_workers\": 2,\n",
    "    \"pin_memory\": True,\n",
    "    \"epochs\": 50,\n",
    "    \"lr\": 1e-1,\n",
    "    \"model\": \"resnet18\",\n",
    "    \"image_size\": 224,\n",
    "    \"use_amp\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05bb8972",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_transforms = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "])\n",
    "\n",
    "train_transforms = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.RandomCrop(32, padding=4),\n",
    "    v2.RandomHorizontalFlip(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    # v2.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
    "])\n",
    "\n",
    "test_transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    # v2.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
    "])\n",
    "\n",
    "train_dataset= CIFAR100(root='./data', train=True, download=True, transform=train_transforms)\n",
    "test_dataset = CIFAR100(root='./data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True, num_workers=config[\"num_workers\"], pin_memory=config[\"pin_memory\"])\n",
    "test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False, num_workers=config[\"num_workers\"], pin_memory=config[\"pin_memory\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29f9983d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act1): ReLU(inplace=True)\n",
       "  (maxpool): Identity()\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
       "  (fc): Linear(in_features=512, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = timm.create_model(config[\"model\"], pretrained=False, num_classes=100)\n",
    "\n",
    "model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "model.maxpool = nn.Identity()\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad3bdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.cuda.amp.autocast(\"cuda\", enabled=config[\"use_amp\"]):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    return train_loss / len(train_loader), 100.0 * correct/total\n",
    "\n",
    "\n",
    "def test(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    return test_loss / len(test_loader), 100.0 * correct/total\n",
    "\n",
    "def train_model(train_loader, test_loader, model, epochs=config[\"epochs\"], batch_size=config[\"batch_size\"], lr=config[\"lr\"]):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "    scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scaler = torch.cuda.amp.GradScaler('cuda', enabled=config[\"use_amp\"])\n",
    "\n",
    "    writer = SummaryWriter(log_dir=f'./logs/cifar100')\n",
    "\n",
    "    best_acc = 0.0\n",
    "\n",
    "    best_loss = None\n",
    "    counter = 0\n",
    "    patience = 10\n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "    print(\"Start training\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        test_loss, test_acc = test(model, test_loader, criterion, device)\n",
    "\n",
    "        scheduler.step()\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        epoch_time = time.time() - epoch_start\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%, LR: {current_lr:.6f}, Time: {epoch_time:.2f}s\")\n",
    "\n",
    "        writer.add_scalar('Loss/Train', train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Test', test_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Train', train_acc, epoch)\n",
    "        writer.add_scalar('Accuracy/Test', test_acc, epoch)\n",
    "        writer.add_scalar('Learning Rate', current_lr, epoch)\n",
    "\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            os.makedirs('./checkpoints', exist_ok=True)\n",
    "\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_acc': best_acc,        \n",
    "            }, f'./checkpoints/cifar_best_model.pth')\n",
    "            print(f\"Best model saved with accuracy: {best_acc:.2f}%\")\n",
    "\n",
    "        if best_loss is None:\n",
    "            best_loss = test_loss\n",
    "        elif test_loss > best_loss:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f\"Early stopping triggered for epoch {epoch+1}\")\n",
    "                break\n",
    "        else:\n",
    "            best_loss = test_loss\n",
    "            counter = 0\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Training completed in {total_time/60:.2f} minutes\")\n",
    "    print(f\"Best Test Accuracy: {best_acc:.2f}%\")\n",
    "    \n",
    "    writer.close()\n",
    "\n",
    "    return {\n",
    "        'best_acc': best_acc,\n",
    "        'total_time': total_time,\n",
    "        'final_train_acc': train_acc,\n",
    "        'final_test_acc': test_acc\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e14e204f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "Epoch 1/50, Train Loss: 3.8425, Train Acc: 10.91%, Test Loss: 3.4793, Test Acc: 16.24%, LR: 0.100000, Time: 43.19s\n",
      "Best model saved with accuracy: 16.24%\n",
      "Epoch 2/50, Train Loss: 3.1362, Train Acc: 22.36%, Test Loss: 3.0392, Test Acc: 24.33%, LR: 0.100000, Time: 44.53s\n",
      "Best model saved with accuracy: 24.33%\n",
      "Epoch 3/50, Train Loss: 2.5881, Train Acc: 32.67%, Test Loss: 2.4548, Test Acc: 35.74%, LR: 0.100000, Time: 44.84s\n",
      "Best model saved with accuracy: 35.74%\n",
      "Epoch 4/50, Train Loss: 2.2893, Train Acc: 39.08%, Test Loss: 2.5421, Test Acc: 34.62%, LR: 0.100000, Time: 45.54s\n",
      "Epoch 5/50, Train Loss: 2.1054, Train Acc: 42.89%, Test Loss: 2.2748, Test Acc: 40.59%, LR: 0.100000, Time: 44.62s\n",
      "Best model saved with accuracy: 40.59%\n",
      "Epoch 6/50, Train Loss: 1.9659, Train Acc: 46.09%, Test Loss: 2.1633, Test Acc: 43.25%, LR: 0.100000, Time: 44.77s\n",
      "Best model saved with accuracy: 43.25%\n",
      "Epoch 7/50, Train Loss: 1.8708, Train Acc: 48.60%, Test Loss: 1.9918, Test Acc: 46.57%, LR: 0.100000, Time: 44.55s\n",
      "Best model saved with accuracy: 46.57%\n",
      "Epoch 8/50, Train Loss: 1.8010, Train Acc: 50.31%, Test Loss: 1.9883, Test Acc: 46.94%, LR: 0.100000, Time: 45.05s\n",
      "Best model saved with accuracy: 46.94%\n",
      "Epoch 9/50, Train Loss: 1.7544, Train Acc: 51.54%, Test Loss: 1.9320, Test Acc: 47.77%, LR: 0.100000, Time: 45.28s\n",
      "Best model saved with accuracy: 47.77%\n",
      "Epoch 10/50, Train Loss: 1.7122, Train Acc: 52.31%, Test Loss: 2.1774, Test Acc: 44.20%, LR: 0.100000, Time: 44.52s\n",
      "Epoch 11/50, Train Loss: 1.6816, Train Acc: 53.03%, Test Loss: 1.8759, Test Acc: 49.11%, LR: 0.100000, Time: 45.21s\n",
      "Best model saved with accuracy: 49.11%\n",
      "Epoch 12/50, Train Loss: 1.6513, Train Acc: 54.11%, Test Loss: 2.6328, Test Acc: 38.41%, LR: 0.100000, Time: 44.49s\n",
      "Epoch 13/50, Train Loss: 1.6294, Train Acc: 54.52%, Test Loss: 2.0691, Test Acc: 46.99%, LR: 0.100000, Time: 44.80s\n",
      "Epoch 14/50, Train Loss: 1.6098, Train Acc: 55.14%, Test Loss: 2.0068, Test Acc: 48.31%, LR: 0.100000, Time: 44.43s\n",
      "Epoch 15/50, Train Loss: 1.5939, Train Acc: 55.52%, Test Loss: 2.0307, Test Acc: 47.92%, LR: 0.100000, Time: 44.68s\n",
      "Epoch 16/50, Train Loss: 1.5855, Train Acc: 55.70%, Test Loss: 2.1463, Test Acc: 44.99%, LR: 0.100000, Time: 44.54s\n",
      "Epoch 17/50, Train Loss: 1.5747, Train Acc: 55.83%, Test Loss: 1.8163, Test Acc: 50.88%, LR: 0.100000, Time: 44.69s\n",
      "Best model saved with accuracy: 50.88%\n",
      "Epoch 18/50, Train Loss: 1.5575, Train Acc: 56.24%, Test Loss: 1.9052, Test Acc: 48.98%, LR: 0.100000, Time: 45.34s\n",
      "Epoch 19/50, Train Loss: 1.5544, Train Acc: 56.45%, Test Loss: 2.0616, Test Acc: 48.26%, LR: 0.100000, Time: 44.63s\n",
      "Epoch 20/50, Train Loss: 1.5410, Train Acc: 56.64%, Test Loss: 2.1625, Test Acc: 43.80%, LR: 0.100000, Time: 45.22s\n",
      "Epoch 21/50, Train Loss: 1.5368, Train Acc: 57.14%, Test Loss: 2.3272, Test Acc: 42.78%, LR: 0.100000, Time: 44.57s\n",
      "Epoch 22/50, Train Loss: 1.5294, Train Acc: 57.30%, Test Loss: 2.0157, Test Acc: 48.00%, LR: 0.100000, Time: 45.74s\n",
      "Epoch 23/50, Train Loss: 1.5251, Train Acc: 57.28%, Test Loss: 1.7567, Test Acc: 52.93%, LR: 0.100000, Time: 44.47s\n",
      "Best model saved with accuracy: 52.93%\n",
      "Epoch 24/50, Train Loss: 1.5158, Train Acc: 57.40%, Test Loss: 1.8727, Test Acc: 49.52%, LR: 0.100000, Time: 45.27s\n",
      "Epoch 25/50, Train Loss: 1.5172, Train Acc: 57.68%, Test Loss: 2.0226, Test Acc: 48.20%, LR: 0.100000, Time: 45.11s\n",
      "Epoch 26/50, Train Loss: 1.5225, Train Acc: 57.53%, Test Loss: 1.8338, Test Acc: 51.07%, LR: 0.100000, Time: 44.81s\n",
      "Epoch 27/50, Train Loss: 1.5056, Train Acc: 57.82%, Test Loss: 1.8537, Test Acc: 50.41%, LR: 0.100000, Time: 45.08s\n",
      "Epoch 28/50, Train Loss: 1.4983, Train Acc: 57.81%, Test Loss: 2.1147, Test Acc: 46.04%, LR: 0.100000, Time: 44.85s\n",
      "Epoch 29/50, Train Loss: 1.4973, Train Acc: 57.89%, Test Loss: 2.2544, Test Acc: 43.96%, LR: 0.100000, Time: 44.63s\n",
      "Epoch 30/50, Train Loss: 1.4947, Train Acc: 57.92%, Test Loss: 1.9170, Test Acc: 49.89%, LR: 0.010000, Time: 45.72s\n",
      "Epoch 31/50, Train Loss: 0.9415, Train Acc: 72.64%, Test Loss: 1.0199, Test Acc: 70.50%, LR: 0.010000, Time: 45.40s\n",
      "Best model saved with accuracy: 70.50%\n",
      "Epoch 32/50, Train Loss: 0.7785, Train Acc: 76.83%, Test Loss: 1.0099, Test Acc: 71.03%, LR: 0.010000, Time: 44.41s\n",
      "Best model saved with accuracy: 71.03%\n",
      "Epoch 33/50, Train Loss: 0.7030, Train Acc: 79.00%, Test Loss: 0.9829, Test Acc: 71.64%, LR: 0.010000, Time: 45.14s\n",
      "Best model saved with accuracy: 71.64%\n",
      "Epoch 34/50, Train Loss: 0.6473, Train Acc: 80.40%, Test Loss: 0.9867, Test Acc: 72.03%, LR: 0.010000, Time: 44.30s\n",
      "Best model saved with accuracy: 72.03%\n",
      "Epoch 35/50, Train Loss: 0.6029, Train Acc: 81.76%, Test Loss: 1.0117, Test Acc: 71.90%, LR: 0.010000, Time: 44.92s\n",
      "Epoch 36/50, Train Loss: 0.5595, Train Acc: 83.03%, Test Loss: 1.0114, Test Acc: 71.94%, LR: 0.010000, Time: 44.24s\n",
      "Epoch 37/50, Train Loss: 0.5368, Train Acc: 83.64%, Test Loss: 1.0160, Test Acc: 72.04%, LR: 0.010000, Time: 45.53s\n",
      "Best model saved with accuracy: 72.04%\n",
      "Epoch 38/50, Train Loss: 0.5061, Train Acc: 84.65%, Test Loss: 1.0226, Test Acc: 71.78%, LR: 0.010000, Time: 44.75s\n",
      "Epoch 39/50, Train Loss: 0.4835, Train Acc: 85.20%, Test Loss: 1.0662, Test Acc: 71.01%, LR: 0.010000, Time: 44.70s\n",
      "Epoch 40/50, Train Loss: 0.4622, Train Acc: 85.54%, Test Loss: 1.0816, Test Acc: 71.11%, LR: 0.010000, Time: 45.43s\n",
      "Epoch 41/50, Train Loss: 0.4549, Train Acc: 85.87%, Test Loss: 1.0606, Test Acc: 71.79%, LR: 0.010000, Time: 44.33s\n",
      "Epoch 42/50, Train Loss: 0.4383, Train Acc: 86.16%, Test Loss: 1.0929, Test Acc: 70.54%, LR: 0.010000, Time: 45.66s\n",
      "Epoch 43/50, Train Loss: 0.4300, Train Acc: 86.58%, Test Loss: 1.1628, Test Acc: 69.83%, LR: 0.010000, Time: 44.19s\n",
      "Early stopping triggered for epoch 43\n",
      "Training completed in 32.19 minutes\n",
      "Best Test Accuracy: 72.04%\n"
     ]
    }
   ],
   "source": [
    "results = train_model(train_loader, test_loader, model, config[\"epochs\"], config[\"batch_size\"], config[\"lr\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
